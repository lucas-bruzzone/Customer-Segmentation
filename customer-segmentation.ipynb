{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lucasbruzzone/using-ica-to-improve-accuracy-0-99-accuracy?scriptVersionId=142749813\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <font color=\"#488000\">Customer Segmentation Classification</font>\n\n## <font color=\"#964400\">Context</font>\n\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4, and P5). After intensive market research, theyâ€™ve deduced that the behavior of the new market is similar to their existing market.\n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D). Then, they performed segmented outreach and communication for a different segment of customers. This strategy has worked exceptionally well for them. They plan to use the same strategy for the new markets and have identified 2627 new potential customers.\n\nYou are required to help the manager to predict the right group of the new customers.\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport os\nimport seaborn as sns\nimport plotly.express as px","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-02T13:14:17.646314Z","iopub.execute_input":"2023-09-02T13:14:17.646698Z","iopub.status.idle":"2023-09-02T13:14:18.279025Z","shell.execute_reply.started":"2023-09-02T13:14:17.646667Z","shell.execute_reply":"2023-09-02T13:14:18.277931Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data\nfilepath = '/kaggle/input/customer-segmentation'\n\ntrain = pd.read_csv(os.path.join(filepath, 'Train.csv'),index_col='ID')\ntest = pd.read_csv(os.path.join(filepath, 'Test.csv'),index_col='ID')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.280201Z","iopub.execute_input":"2023-09-02T13:14:18.280629Z","iopub.status.idle":"2023-09-02T13:14:18.320435Z","shell.execute_reply.started":"2023-09-02T13:14:18.280598Z","shell.execute_reply":"2023-09-02T13:14:18.319341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#964400\">Content</font> \n\n**Variable** | **Definition**\n--- | ---\nID | Unique ID\nGender | Gender of the customer\nEver_Married | Marital status of the customer\nAge | Age of the customer\nGraduated | Is the customer a graduate?\nProfession | Profession of the customer\nWork_Experience | Work Experience in years\nSpending_Score | Spending score of the customer\nFamily_Size | Number of family members for the customer (including the customer)\nVar_1 | Anonymised Category for the customer\nSegmentation | (target) Customer Segment of the customer\n","metadata":{}},{"cell_type":"code","source":"def set_frame_style(df, caption=\"\"):\n    \"\"\"Helper function to set dataframe presentation style.\n    \"\"\"\n    return df.style.background_gradient(cmap='Greens').set_caption(caption).set_table_styles([{\n    'selector': 'caption',\n    'props': [\n        ('color', 'Green'),\n        ('font-size', '18px'),\n        ('font-weight','bold')\n    ]}])\n\ndef check_data(data, title):\n    cols = data.columns.to_list()\n    display(set_frame_style(data[cols].head(), f'{title}:First Five rows'))\n    display(set_frame_style(data[cols].nunique().to_frame().rename({0:'Unique Value Count'}, axis=1).transpose(), f'{title}: Unique Value Counts In Each Column'))\n    display(set_frame_style(data[cols].isna().sum().to_frame().transpose(), f'{title}:Columns With Nan'))","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.322024Z","iopub.execute_input":"2023-09-02T13:14:18.32281Z","iopub.status.idle":"2023-09-02T13:14:18.331168Z","shell.execute_reply.started":"2023-09-02T13:14:18.32276Z","shell.execute_reply":"2023-09-02T13:14:18.329727Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_data(train,\"Train Data\")","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.336032Z","iopub.execute_input":"2023-09-02T13:14:18.336415Z","iopub.status.idle":"2023-09-02T13:14:18.458646Z","shell.execute_reply.started":"2023-09-02T13:14:18.336353Z","shell.execute_reply":"2023-09-02T13:14:18.457188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#964400\">Replacing NaN values</font>\n\nThe following columns contain NaN values:\n\n**Variable** | **Treatment**\n--- | ---\nEver_Married | Replace NaN with \"No\" (assuming \"No\" means that the customer is not married)\nGraduated | Replace NaN with \"No\" (assuming \"No\" means that the customer is not graduated)\nProfession | Replace NaN with \"Not Specified\" (unspecified)\nWork_Experience | Replace NaN with 0 (zero, indicating that the customer has no work experience)\nFamily_Size | Replace NaN with the mean of valid values in this column\nVar_1 | Replace NaN with \"Unknown\"\n","metadata":{}},{"cell_type":"code","source":"def handle_missing_values(df):\n    # Replace NaN in Ever_Married with \"No\"\n    df['Ever_Married'].fillna('No', inplace=True)\n\n    # Replace NaN in Graduated with \"No\"\n    df['Graduated'].fillna('No', inplace=True)\n\n    # Replace NaN in Profession with \"Not Specified\"\n    df['Profession'].fillna('Not Specified', inplace=True)\n\n    # Replace NaN in Work_Experience with 0\n    df['Work_Experience'].fillna(0, inplace=True)\n\n    # Replace NaN in Family_Size with the mean of valid values\n    mean_family_size = round(df['Family_Size'].mean())\n    df['Family_Size'].fillna(mean_family_size, inplace=True)\n\n    # Replace NaN in Var_1 with \"Unknown\"\n    df['Var_1'].fillna('Unknown', inplace=True)\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.460298Z","iopub.execute_input":"2023-09-02T13:14:18.460632Z","iopub.status.idle":"2023-09-02T13:14:18.468687Z","shell.execute_reply.started":"2023-09-02T13:14:18.460606Z","shell.execute_reply":"2023-09-02T13:14:18.4674Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = handle_missing_values(train)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.469689Z","iopub.execute_input":"2023-09-02T13:14:18.46995Z","iopub.status.idle":"2023-09-02T13:14:18.499169Z","shell.execute_reply.started":"2023-09-02T13:14:18.469926Z","shell.execute_reply":"2023-09-02T13:14:18.498414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#488000\">Exploratory Data Analysis (EDA)</font>\n\n\nAfter completing the necessary data treatments for all columns, we are ready to embark on the Exploratory Data Analysis (EDA) phase. EDA is a critical step in the data analysis process that helps us gain a deeper understanding of the dataset and its underlying patterns. It involves various statistical and visual techniques to explore the data, identify trends, detect anomalies, and generate insights.\n","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"#964400\">Data visualization</font>","metadata":{}},{"cell_type":"code","source":"def visualize_data(df, column1, column2=None):\n    if column2 is None:\n        # Create a histogram for a single column\n        fig = px.histogram(df, x=column1, title=f'{column1}')\n    else:\n        # Check if column1 and column2 are numerical\n        if pd.api.types.is_numeric_dtype(df[column1]) and pd.api.types.is_numeric_dtype(df[column2]):\n            # Create a scatter plot for two numerical columns\n            fig = px.scatter(df, x=column1, y=column2, title=f'{column1} vs {column2}')\n        else:\n            # Create a box plot for a categorical column vs. a numerical column\n            fig = px.box(df, x=column1, y=column2, title=f'{column1} vs {column2}')\n\n    # Show the figure\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.501381Z","iopub.execute_input":"2023-09-02T13:14:18.501729Z","iopub.status.idle":"2023-09-02T13:14:18.509139Z","shell.execute_reply.started":"2023-09-02T13:14:18.501697Z","shell.execute_reply":"2023-09-02T13:14:18.50782Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#964400\">Histogram 'Age'</font>\n\n- All customers are above 18 years old.\n- There is a certain average of customers within each ten-year age range. To simplify, we can create an age range column like 20-30, 31-40, 41-50, etc.","metadata":{}},{"cell_type":"code","source":"# Histogram of Age\nvisualize_data(train, 'Age')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.510774Z","iopub.execute_input":"2023-09-02T13:14:18.511084Z","iopub.status.idle":"2023-09-02T13:14:18.774288Z","shell.execute_reply.started":"2023-09-02T13:14:18.511057Z","shell.execute_reply":"2023-09-02T13:14:18.773131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#964400\">'Age' vs 'Spending_Score'</font>\n\n- It's important to note that the categories 'Low,' 'Average,' and 'High' are well-defined by age. On average, 'Low' is around 35 years old, 'Average' is around 46 years old, and 'High' is around 57.5 years old.\n","metadata":{}},{"cell_type":"code","source":"# Scatter Plot of Age vs. Spending_Score\nvisualize_data(train, 'Age', 'Spending_Score')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.775616Z","iopub.execute_input":"2023-09-02T13:14:18.775953Z","iopub.status.idle":"2023-09-02T13:14:18.849675Z","shell.execute_reply.started":"2023-09-02T13:14:18.775928Z","shell.execute_reply":"2023-09-02T13:14:18.848895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#964400\">'Age' vs 'Var_1'</font>\n\n- Some noticeable differences between 'cat_6' and 'unknown.' In the rest of the categories, patterns seem to follow a similar trend by age.\n","metadata":{}},{"cell_type":"code","source":"# Box Plot of Var_1 vs. Age\nvisualize_data(train, 'Age', 'Var_1')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.850718Z","iopub.execute_input":"2023-09-02T13:14:18.851026Z","iopub.status.idle":"2023-09-02T13:14:18.919555Z","shell.execute_reply.started":"2023-09-02T13:14:18.850998Z","shell.execute_reply":"2023-09-02T13:14:18.918757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#964400\">'Age' vs 'Segmentation'</font>\n\n- It's important to highlight that the segments have visibly different age averages:\n  - Segment A: 41 years\n  - Segment B: 46 years\n  - Segment C: 49 years\n  - Segment D: 29 years\n- Segments B and C are quite similar in terms of age distribution.","metadata":{}},{"cell_type":"code","source":"# Box Plot of Segmentation vs. Age\nvisualize_data(train, 'Age', 'Segmentation')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.920658Z","iopub.execute_input":"2023-09-02T13:14:18.920929Z","iopub.status.idle":"2023-09-02T13:14:18.992778Z","shell.execute_reply.started":"2023-09-02T13:14:18.920904Z","shell.execute_reply":"2023-09-02T13:14:18.991799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#964400\">Label Encoding</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_categorical_features(df):\n    # Separate numeric and categorical columns, excluding the target column\n    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n\n    label_encoder_mappings = {}\n    le = LabelEncoder()\n\n    # Encode categorical columns\n    for col in cat_cols:\n        df[col] = le.fit_transform(df[col])\n        label_encoder_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n\n    return df, label_encoder_mappings\n\ntrain, label_encoder_mappings = encode_categorical_features(train)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:18.993893Z","iopub.execute_input":"2023-09-02T13:14:18.994178Z","iopub.status.idle":"2023-09-02T13:14:19.057647Z","shell.execute_reply.started":"2023-09-02T13:14:18.994151Z","shell.execute_reply":"2023-09-02T13:14:19.056406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#964400\">Correlation Analysis</font>","metadata":{}},{"cell_type":"code","source":"def plot_correlation_heatmap(df, target_column):\n    # Calculate correlation matrix for numeric columns\n    corr = df.corr(numeric_only=True)\n    \n    # Calculate correlation with the target column\n    target_corr = corr[target_column].drop(target_column)\n    \n    # Sort correlation values in descending order\n    target_corr_sorted = target_corr.sort_values(ascending=False)\n    \n    # Select features with correlation values greater than or equal to 0.05 in absolute value\n    target_corr_filtered = target_corr_sorted[abs(target_corr_sorted) >= 0.05]\n    \n    # Set the style and palette for the heatmap\n    sns.set(font_scale=0.8)\n    sns.set_style(\"white\")\n    sns.set_palette(\"PuBuGn_d\")\n    \n    # Create a heatmap with the selected features\n    sns.heatmap(target_corr_filtered.to_frame(), cmap=\"coolwarm\", annot=True, fmt='.2f')\n    plt.title(f'Correlation with {target_column}')\n    plt.show()\n    \n    # Return the DataFrame with the selected features\n    return target_corr_filtered.index.tolist()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:19.059216Z","iopub.execute_input":"2023-09-02T13:14:19.059601Z","iopub.status.idle":"2023-09-02T13:14:19.068166Z","shell.execute_reply.started":"2023-09-02T13:14:19.059567Z","shell.execute_reply":"2023-09-02T13:14:19.066414Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_corr_filtered = plot_correlation_heatmap(train, 'Segmentation')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:19.072311Z","iopub.execute_input":"2023-09-02T13:14:19.072979Z","iopub.status.idle":"2023-09-02T13:14:19.372547Z","shell.execute_reply.started":"2023-09-02T13:14:19.072946Z","shell.execute_reply":"2023-09-02T13:14:19.371127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#964400\">Features with Significant Correlation</font>\n\nWhen conducting a correlation analysis between the dataset's features and the target variable 'Segmentation,' we have identified several features that exhibit a significant correlation (greater than 0.05) with customer segmentation. These features can be particularly important for understanding and predicting customer segments. Let's highlight some of them:\n\n1. **<font color=\"#964400\">Profession (0.20)</font>**:\n   - The 'Profession' feature shows a positive correlation of 0.20 with customer segmentation. This suggests that customers' professions may influence their market segment. For example, certain professions may be associated with different income levels, needs, and purchasing behaviors.\n\n2. **<font color=\"#964400\">Family_Size (0.19)</font>**:\n   - The 'Family_Size' characteristic has a positive correlation of 0.19 with segmentation. This indicates that family size may play a role in determining the market segment. Customer segments with larger families may have distinct needs and preferences compared to smaller families.\n\n3. **<font color=\"#964400\">Spending_Score (0.10)</font>**:\n   - The 'Spending_Score' feature exhibits a positive correlation of 0.10 with segmentation. This suggests that customers' spending behavior is relevant to defining their segments. Customers with different spending scores may be directed to specific segments based on their purchasing power.\n\n4. **<font color=\"#964400\">Graduated (-0.18)</font>**:\n   - 'Graduated' presents a negative correlation of -0.18. This implies that completing graduation can significantly influence a customer's market segment. Customers with an academic background may belong to different segments compared to those without formal education.\n\n5. **<font color=\"#964400\">Ever_Married (-0.22)</font>**:\n   - The 'Ever_Married' feature has a negative correlation of -0.22. This indicates that customers' marital status plays a crucial role in determining the segment. Married or unmarried individuals may have distinct needs and purchasing behaviors.\n\n6. **<font color=\"#964400\">Age (-0.24)</font>**:\n   - The 'Age' feature exhibits a negative correlation of -0.24. This suggests that customers' age has a substantial impact on their segmentation. Different age groups can be grouped into specific segments due to varied consumption behaviors throughout life.\n\nThese features with significant correlations can be valuable for developing targeted marketing strategies for different customer segments based on their individual characteristics. They provide useful insights for understanding customer behavior and can aid in making more informed decisions regarding business strategy.</font>\n","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"#964400\">Scatter Analysis</font>\n\nIn this analysis, we explore the use of scatter plots in understanding the distribution of data points in our dataset. Scatter plots are an essential tool for visualizing relationships and patterns in data points, especially in multidimensional datasets. For our analysis, we utilized scatter plots to gain insights into the distribution of data points in two different scenarios: the complete training dataset and a reduced dataset with selected features. The goal was to highlight the importance of feature selection in improving the interpretability and clarity of scatter plots.\n\n### Full Training Dataset\n\nIn the first scenario, we created scatter plots using the complete training dataset. This dataset contains a wide range of features, some of which may not be highly relevant for our analysis. While scatter plots can provide valuable insights, visualizing data in high dimensions can be overwhelming, making it challenging to identify meaningful patterns.\n\nDespite the challenges presented by the full dataset, we were able to observe some interesting trends and clusters in the scatter plots. These insights provided a starting point for our analysis and allowed us to identify areas of interest.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA, NMF, FastICA\nfrom mpl_toolkits.mplot3d import Axes3D\n\nclass Decomp:\n    def __init__(self, n_components, method=\"pca\", scaler_method='standard'):\n        self.n_components = n_components\n        self.method = method\n        self.scaler_method = scaler_method\n        self.mixing_ = None\n        \n    def dimension_reduction(self, df):\n        X_reduced = self.dimension_method(df)\n        if self.n_components == 2:\n            df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        elif self.n_components == 3:\n            df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        return df_comp\n    \n    def dimension_method(self, df):\n        X = self.scaler(df)\n        if self.method == \"pca\":\n            pca = PCA(n_components=self.n_components, random_state=0)\n            X_reduced = pca.fit_transform(X)\n            self.comp = pca\n        elif self.method == \"nmf\":\n            nmf = NMF(n_components=self.n_components, random_state=0)\n            X_reduced = nmf.fit_transform(X)\n        elif self.method == \"ica\":\n            comp = FastICA(n_components=self.n_components, whiten='unit-variance', random_state=0)\n            X_reduced = comp.fit_transform(X)\n            self.mixing_ = comp.mixing_  # Store the mixing_ matrix\n        else:\n            raise ValueError(f\"Invalid method name: {method}\")\n        \n        \n        return X_reduced\n    \n    def get_mixing_matrix(self):\n        if self.method == \"ica\":\n            return self.mixing_\n        else:\n            raise ValueError(\"Mixing matrix is only available for ICA dimensionality reduction method.\")\n    \n    \n    def scaler(self, df):\n        _df = df.copy()\n        if self.scaler_method == \"standard\":\n            return StandardScaler().fit_transform(_df)\n        elif self.scaler_method == \"minmax\":\n            return MinMaxScaler().fit_transform(_df)\n        elif self.scaler_method == None:\n            return _df.values\n        else:\n            raise ValueError(f\"Invalid scaler_method name\")\n        \n    def get_columns(self):\n        return [f'{self.method.upper()}_{_}' for _ in range(self.n_components)]\n    \n    def get_explained_variance_ratio(self):\n        return np.sum(self.comp.explained_variance_ratio_)\n    \n    def transform(self, df):\n        X = self.scaler(df)\n        X_reduced = self.comp.transform(X)\n        if self.n_components == 2:\n            df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        elif self.n_components == 3:\n            df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        return df_comp\n    \n    def decomp_plot(self, tmp, label, hue='genre'):\n        if self.n_components == 2:\n            # GrÃ¡fico 2D\n            plt.figure(figsize=(16, 9))\n            sns.scatterplot(x=f\"{label}_0\", y=f\"{label}_1\", data=tmp, hue=hue, alpha=0.7, s=100, palette='coolwarm');\n            plt.title(f'{label} on {hue}', fontsize=20)\n            plt.xticks(fontsize=14)\n            plt.yticks(fontsize=10)\n            plt.xlabel(f\"{label} Component 1\", fontsize=15)\n            plt.ylabel(f\"{label} Component 2\", fontsize=15)\n        elif self.n_components == 3:\n            # GrÃ¡fico 3D\n            fig = plt.figure(figsize=(16, 9))\n            ax = fig.add_subplot(111, projection='3d')\n            scatter = ax.scatter(tmp[f\"{label}_0\"], tmp[f\"{label}_1\"], tmp[f\"{label}_2\"], c=tmp[hue], cmap='coolwarm', s=100)\n            ax.set_xlabel(f\"{label} Component 1\", fontsize=15)\n            ax.set_ylabel(f\"{label} Component 2\", fontsize=15)\n            ax.set_zlabel(f\"{label} Component 3\", fontsize=15)\n            plt.title(f'{label} on {hue} (3D)', fontsize=20)\n            fig.colorbar(scatter)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:19.37374Z","iopub.execute_input":"2023-09-02T13:14:19.374027Z","iopub.status.idle":"2023-09-02T13:14:19.595776Z","shell.execute_reply.started":"2023-09-02T13:14:19.374002Z","shell.execute_reply":"2023-09-02T13:14:19.594556Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train.copy()\nmethod = 'ica'\n\ndecomp = Decomp(n_components=3, method=method, scaler_method='minmax')\ndecomp_feature = decomp.dimension_reduction(data)\ndecomp_feature = pd.concat([train['Segmentation'], decomp_feature], axis=1)\ndecomp.decomp_plot(decomp_feature, method.upper(), 'Segmentation')\n\ndel data","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:19.597226Z","iopub.execute_input":"2023-09-02T13:14:19.597515Z","iopub.status.idle":"2023-09-02T13:14:20.144049Z","shell.execute_reply.started":"2023-09-02T13:14:19.597491Z","shell.execute_reply":"2023-09-02T13:14:20.142827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection for Improved Insights\n\nTo enhance the interpretability of our scatter analysis, we performed feature selection. This step involved choosing a subset of features that were expected to have a more significant impact on the distribution of data points. By reducing the dimensionality of the dataset, we aimed to create scatter plots that were more focused and easier to interpret.\n\nThe feature selection process led to a reduced training dataset with selected features. This streamlined dataset allowed us to create scatter plots that highlighted specific relationships and patterns more effectively.","metadata":{}},{"cell_type":"code","source":"train_filtered = train[target_corr_filtered + ['Segmentation']]","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:20.145311Z","iopub.execute_input":"2023-09-02T13:14:20.1456Z","iopub.status.idle":"2023-09-02T13:14:20.151643Z","shell.execute_reply.started":"2023-09-02T13:14:20.145578Z","shell.execute_reply":"2023-09-02T13:14:20.150882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_filtered.copy()\nmethod = 'ica'\n\ndecomp = Decomp(n_components=3, method=method, scaler_method='minmax')\ndecomp_feature = decomp.dimension_reduction(data)\ndecomp_feature = pd.concat([train_filtered['Segmentation'], decomp_feature], axis=1)\ndecomp.decomp_plot(decomp_feature, method.upper(), 'Segmentation')   \ndel data","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:20.152904Z","iopub.execute_input":"2023-09-02T13:14:20.153456Z","iopub.status.idle":"2023-09-02T13:14:20.726982Z","shell.execute_reply.started":"2023-09-02T13:14:20.153424Z","shell.execute_reply":"2023-09-02T13:14:20.726286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ica = decomp_feature.copy()\n#mixing_matrix = decomp.get_mixing_matrix()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:20.72825Z","iopub.execute_input":"2023-09-02T13:14:20.728739Z","iopub.status.idle":"2023-09-02T13:14:20.733898Z","shell.execute_reply.started":"2023-09-02T13:14:20.728708Z","shell.execute_reply":"2023-09-02T13:14:20.732512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Improved Interpretation\n\nThe comparison between the scatter plots of the full training dataset and the reduced dataset was striking. In the reduced dataset, the relationships between variables became more evident, clusters were more defined, and patterns were easier to discern. This improvement in interpretation demonstrated the importance of feature selection in data analysis.","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"#964400\">Class Distribution</font>\n\nAnalyzing class distribution is a crucial aspect when dealing with customer segmentation datasets. It provides valuable insights into how different classes or segments are distributed and whether there's any significant imbalance among them. In the case of this customer segmentation dataset, it's important to note that it exhibits a homogeneous class distribution.\n\nThis means that the customer classes or segments (A, B, C, and D) have a nearly equal representation within the dataset. This homogeneous distribution is advantageous because it avoids imbalances that can hinder a machine learning model's ability to generalize across all classes. In other words, all classes have reasonably similar amounts of examples, allowing the model to learn in a more balanced manner.\n\nA dataset with a homogeneous class distribution is ideal for training classification models as it helps prevent bias and ensures the model can make fair and accurate decisions for all customer classes.\n\nTherefore, while exploring this dataset, we can have confidence that the balanced class distribution will contribute to more reliable analysis and more accurate results when developing customer segmentation models.</font>\n","metadata":{}},{"cell_type":"code","source":"def plot_target_feature(df, target_col, label_encoder_mappings, figsize=(16,5), palette='colorblind', name='Train'):\n    df = df.fillna('Nan')\n    df = df.sort_values(target_col)\n\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n    ax = ax.flatten()\n\n    # Pie chart\n    pie_colors = sns.color_palette(palette, len(df[target_col].unique()))\n    ax[0].pie(\n        df[target_col].value_counts(),\n        shadow=True,\n        explode=[0.05] * len(df[target_col].unique()),\n        autopct='%1.f%%',\n        textprops={'size': 15, 'color': 'white'},\n        colors=pie_colors\n    )\n    ax[0].set_aspect('equal')  # Fix the aspect ratio to make the pie chart circular\n\n    # Bar plot\n    bar_colors = sns.color_palette(palette)\n    sns.countplot(\n        data=df,\n        y=target_col,\n        ax=ax[1],\n        palette=bar_colors\n    )\n    ax[1].set_xlabel('Count', fontsize=14)\n    ax[1].set_ylabel('')\n    ax[1].tick_params(labelsize=12)\n    ax[1].yaxis.set_tick_params(width=0)  # Remove tick lines for y-axis\n\n    target_col_label = label_encoder_mappings[target_col]\n    fig.suptitle(f'{target_col_label} in {name} Dataset', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:20.735315Z","iopub.execute_input":"2023-09-02T13:14:20.735668Z","iopub.status.idle":"2023-09-02T13:14:20.750976Z","shell.execute_reply.started":"2023-09-02T13:14:20.735634Z","shell.execute_reply":"2023-09-02T13:14:20.749975Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_target_feature(train_ica, 'Segmentation', label_encoder_mappings, figsize=(16,5), palette='colorblind', name='Train data')","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:20.752084Z","iopub.execute_input":"2023-09-02T13:14:20.753045Z","iopub.status.idle":"2023-09-02T13:14:21.13054Z","shell.execute_reply.started":"2023-09-02T13:14:20.752958Z","shell.execute_reply":"2023-09-02T13:14:21.129254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#964400\">Model Training and Evaluation</font>\n\nIn this code snippet, we are training and evaluating machine learning models on a preprocessed dataset (`train_pca`) to predict the target variable <font color=\"#964400\">`Segmentation`</font>. The goal is to assess the performance of different classifiers.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef train_and_evaluate_models(df, target_col, df_name):\n    # Separate features and the target column\n    X = df.drop(target_col, axis=1).values\n    y = df[target_col].values\n    n_classes = len(np.unique(y))\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Initialize a list to store the trained models and their accuracies\n    trained_models = []\n\n    # Random Forest\n    random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    random_forest_model.fit(X_train, y_train)\n    random_forest_predictions = random_forest_model.predict(X_test)\n    random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)\n    print(f\"{df_name} - Random Forest Accuracy: {random_forest_accuracy:.2f}\")\n\n    # C5 (Decision Tree)\n    c5_model = DecisionTreeClassifier(random_state=42)\n    c5_model.fit(X_train, y_train)\n    c5_predictions = c5_model.predict(X_test)\n    c5_accuracy = accuracy_score(y_test, c5_predictions)\n    print(f\"{df_name} - C5 Accuracy: {c5_accuracy:.2f}\")\n\n    # HistGradientBoostingClassifier\n    hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=42)\n    hist_gradient_boosting_model.fit(X_train, y_train)\n    hist_gradient_boosting_predictions = hist_gradient_boosting_model.predict(X_test)\n    hist_gradient_boosting_accuracy = accuracy_score(y_test, hist_gradient_boosting_predictions)\n    print(f\"{df_name} - HistGradientBoostingClassifier Accuracy: {hist_gradient_boosting_accuracy:.2f}\")\n\n    # KNN (K-Nearest Neighbors)\n    knn_model = KNeighborsClassifier(n_neighbors=5)\n    knn_model.fit(X_train, y_train)\n    knn_predictions = knn_model.predict(X_test)\n    knn_accuracy = accuracy_score(y_test, knn_predictions)\n    print(f\"{df_name} - KNN Accuracy: {knn_accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:21.132539Z","iopub.execute_input":"2023-09-02T13:14:21.133312Z","iopub.status.idle":"2023-09-02T13:14:21.165294Z","shell.execute_reply.started":"2023-09-02T13:14:21.133271Z","shell.execute_reply":"2023-09-02T13:14:21.16361Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_evaluate_models(train_ica,'Segmentation', \"Train data Filtered\")","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:21.16685Z","iopub.execute_input":"2023-09-02T13:14:21.16732Z","iopub.status.idle":"2023-09-02T13:14:22.922984Z","shell.execute_reply.started":"2023-09-02T13:14:21.167281Z","shell.execute_reply":"2023-09-02T13:14:22.922064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#964400\">Performance Evaluation</font>\nWe applied a set of machine learning algorithms to predict the <font color=\"#964400\">`Segmentation`</font> target variable based on a preprocessed dataset. Let's take a look at the accuracy scores obtained from each algorithm:\n\n- Random Forest: This ensemble model, which combines multiple decision trees, demonstrated strong predictive power.\n\n- C5 (Decision Tree): While not as high as the Random Forest, this single decision tree model still provided a respectable performance.\n\n- HistGradientBoostingClassifier: This boosting algorithm showcased excellent predictive capabilities.\n\n- K-Nearest Neighbors (KNN): This model, which relies on nearest neighbor information, delivered competitive results.\n\nOverall, these algorithms have demonstrated strong predictive abilities on the dataset. The Random Forest model stood out with the highest accuracy, indicating its effectiveness in solving the classification task. However, all the models performed reasonably well, showcasing their potential for various machine learning applications.","metadata":{}},{"cell_type":"markdown","source":"# <font color=\"#488000\">Data Preprocessing for Model Evaluation</font>\n\nBefore we proceed with evaluating our trained machine learning models, it's essential to ensure that the test data is in a format consistent with the data used for training. This consistency is crucial to obtain reliable performance metrics when assessing the models' accuracy.\n","metadata":{}},{"cell_type":"code","source":"# Handle missing values in the test data\ntest = handle_missing_values(test)\n# Encode categorical features in the test data and store label encoding mappings\ntest, label_encoder_mappings = encode_categorical_features(test)","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:22.924131Z","iopub.execute_input":"2023-09-02T13:14:22.924376Z","iopub.status.idle":"2023-09-02T13:14:22.942357Z","shell.execute_reply.started":"2023-09-02T13:14:22.924352Z","shell.execute_reply":"2023-09-02T13:14:22.941151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only the relevant features (including target 'Segmentation') based on correlation\ntest_filtered = test[target_corr_filtered + ['Segmentation']]\n\n# Perform dimensionality reduction on the test data\ndata = test_filtered.copy()\nmethod = 'ica'\n\ndecomp = Decomp(n_components=3, method=method, scaler_method='minmax')\ndecomp_feature = decomp.dimension_reduction(data)\ndecomp_feature = pd.concat([test_filtered['Segmentation'], decomp_feature], axis=1)\ndecomp.decomp_plot(decomp_feature, method.upper(), 'Segmentation')\n\ndel data\n\ntest_ica = decomp_feature.copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:22.943879Z","iopub.execute_input":"2023-09-02T13:14:22.944259Z","iopub.status.idle":"2023-09-02T13:14:23.402406Z","shell.execute_reply.started":"2023-09-02T13:14:22.944225Z","shell.execute_reply":"2023-09-02T13:14:23.401559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate_models_train_test(train_df, test_df, target_col, df_name):\n    # Separate features and the target column for training and test sets\n    X_train = train_df.drop(target_col, axis=1).values\n    y_train = train_df[target_col].values\n    X_test = test_df.drop(target_col, axis=1).values\n    y_test = test_df[target_col].values\n    \n    # Initialize a list to store the trained models and their accuracies\n    trained_models = []\n\n    # Random Forest\n    random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    random_forest_model.fit(X_train, y_train)\n    random_forest_predictions = random_forest_model.predict(X_test)\n    random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)\n    print(f\"{df_name} - Random Forest Accuracy: {random_forest_accuracy:.2f}\")\n    trained_models.append((\"Random Forest\", random_forest_model, random_forest_accuracy))\n\n    # C5 (Decision Tree)\n    c5_model = DecisionTreeClassifier(random_state=42)\n    c5_model.fit(X_train, y_train)\n    c5_predictions = c5_model.predict(X_test)\n    c5_accuracy = accuracy_score(y_test, c5_predictions)\n    print(f\"{df_name} - C5 Accuracy: {c5_accuracy:.2f}\")\n    trained_models.append((\"C5\", c5_model, c5_accuracy))\n\n    # HistGradientBoostingClassifier\n    hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=42)\n    hist_gradient_boosting_model.fit(X_train, y_train)\n    hist_gradient_boosting_predictions = hist_gradient_boosting_model.predict(X_test)\n    hist_gradient_boosting_accuracy = accuracy_score(y_test, hist_gradient_boosting_predictions)\n    print(f\"{df_name} - HistGradientBoostingClassifier Accuracy: {hist_gradient_boosting_accuracy:.2f}\")\n    trained_models.append((\"HistGradientBoostingClassifier\", hist_gradient_boosting_model, hist_gradient_boosting_accuracy))\n\n    # KNN (K-Nearest Neighbors)\n    knn_model = KNeighborsClassifier(n_neighbors=5)\n    knn_model.fit(X_train, y_train)\n    knn_predictions = knn_model.predict(X_test)\n    knn_accuracy = accuracy_score(y_test, knn_predictions)\n    print(f\"{df_name} - KNN Accuracy: {knn_accuracy:.2f}\")\n    trained_models.append((\"KNN\", knn_model, knn_accuracy))\n\n    return trained_models","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:23.403522Z","iopub.execute_input":"2023-09-02T13:14:23.404633Z","iopub.status.idle":"2023-09-02T13:14:23.416018Z","shell.execute_reply.started":"2023-09-02T13:14:23.404598Z","shell.execute_reply":"2023-09-02T13:14:23.414761Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have 'train' as the training dataset and 'test' as the test dataset\ntrained_models = train_and_evaluate_models_train_test(train_ica, test_ica, 'Segmentation', \"Train data vs Test data\")","metadata":{"execution":{"iopub.status.busy":"2023-09-02T13:14:23.417446Z","iopub.execute_input":"2023-09-02T13:14:23.417773Z","iopub.status.idle":"2023-09-02T13:14:25.430796Z","shell.execute_reply.started":"2023-09-02T13:14:23.417744Z","shell.execute_reply":"2023-09-02T13:14:25.429987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#488000\">Customer Segmentation Prediction - Conclusion</font>\n\nIn this customer segmentation prediction project, we applied various machine learning models to classify customers into different segments based on their characteristics. After a thorough analysis of the results, we've come to some important conclusions.\n\n## <font color=\"#964400\">Model Evaluation</font>\n\nWe evaluated the models using both the training and test datasets. Here are the accuracy scores for each model on the test data:\n\n- Random Forest: 0.98\n- C5 (Decision Tree): 0.93\n- HistGradientBoostingClassifier: 0.99\n- KNN (K-Nearest Neighbors): 0.93\n\nThe accuracy scores are impressive, indicating that our models are performing exceptionally well in classifying customers into segments. However, accuracy alone may not be the only factor to consider when choosing the best model.\n\n## <font color=\"#964400\">Model Selection</font>\n\nWhen selecting the best model for customer segmentation, we need to consider factors beyond just accuracy. The choice of the most suitable model depends on the specific requirements of the business problem and the interpretability of the model.\n\n- **Random Forest**: Achieved the highest accuracy of 0.98. It's a powerful ensemble model that provides good results. However, it may lack interpretability compared to simpler models.\n\n- **C5 (Decision Tree)**: Achieved an accuracy of 0.93. Decision trees are known for their interpretability, making it easier to understand how predictions are made.\n\n- **HistGradientBoostingClassifier**: Achieved the highest accuracy of 0.99. This model combines the strengths of gradient boosting with histogram-based learning, offering high accuracy and good interpretability.\n\n- **KNN (K-Nearest Neighbors)**: Achieved an accuracy of 0.93. KNN is straightforward and interpretable, but it might not perform as well as ensemble models in complex datasets.\n\nConsidering the high accuracy and interpretability, we choose the **HistGradientBoostingClassifier** as the model to build personas for each class.\n\n## <font color=\"#964400\">Dimensionality Reduction Technique</font>\n\nOne key aspect of our analysis was the reduction of dimensionality. We utilized Independent Component Analysis (ICA) to transform our data into a lower-dimensional space. ICA was chosen for its ability to capture independent components in the data.\n\nBy reducing dimensionality using ICA, we simplified the dataset while retaining its essential information. This allowed our models to perform efficiently and effectively, as reflected in the high accuracy scores.\n\nIn conclusion, the combination of the powerful **HistGradientBoostingClassifier** model and effective dimensionality reduction techniques, particularly ICA, enabled us to successfully segment customers with high accuracy. The choice of the best model aligns with the specific business needs and interpretability requirements.\n","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}